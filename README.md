# OptimisedResNet
This Repository contains the experiments and implementation of optimized model of ResNet18
## Abstract
The Residual Networks \textit{(ResNet)} were developed to bring together the advantage of having a deeper network and simultaneously solving the problem of vanishing gradient. The ResNets tackle the problem of vanishing gradients by deploying skip connections, which skip some layers in the neural network and feed one layer's output as the input to the subsequent layers. This helps the gradients flow directly backward from later layers to the initial filter during backpropagation, thereby mitigating the network's vanishing gradients. ResNet18 belongs to the family of ResNets, comprising 18 deep layers and approximately 11 million trainable parameters. In our work shown in this report, we try to maximize the test accuracy of the ResNet18 model by keeping a constraint where the number of trainable parameters of the new architecture should not exceed 5 million. We experiment on the ResNet18 model by modifying its layer structure and subjecting it to various parameters and optimizers. Finally, inferring from the experimental results, we propose a modified architecture of ResNet18, which exhibits the highest test accuracy,  given the constraint.
